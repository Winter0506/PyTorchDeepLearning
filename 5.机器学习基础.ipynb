{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3993, 0.6961])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.4 选择合适的激活函数\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "input = torch.randn(2)\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6014, -1.0122]], requires_grad=True)\n",
      "tensor([[-0.3023, -1.2277]])\n",
      "tensor(0.0680, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 5.5 选择合适的损失函数\n",
    "# 均方误差损失\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(10)\n",
    "loss = nn.MSELoss(reduction='mean')\n",
    "input = torch.randn(1,2, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.randn(1,2)\n",
    "print(target)\n",
    "output = loss(input,target)\n",
    "print(output)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4795, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交叉熵损失\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(10)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 假设类别数目为5\n",
    "input = torch.randn(3,5,requires_grad=True)\n",
    "# 每个样本对应的类别索引，其值范围为[0,4]\n",
    "target = torch.empty(3,dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 GPU加速\n",
    "# 把数据从内存转移到GPU，一般针对张量（我们需要的数据）和模型。\n",
    "# 对张量（类型为FloatTensor或者是LongTensor等），一律直接使用方法.to(device)或.cuda()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 或者 device = torch.device(\"cuda:0\")\n",
    "# device1 = torch.device(\"cuda:1\")\n",
    "for batch_idx, (img, label) in enumerate(train_loader):\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于模型来说，也是同样的方式，使用.to(device)或.cuda来将网络放到GPU显存\n",
    "# 实例化网络\n",
    "model = Net()\n",
    "model.to(device) # 使用序号为0的gpu\n",
    "# model.to(device1) # 使用序号为1的gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7.2 多gpu加速\n",
    "# 单机多GPU主要采用了DataParallel函数，而不是DistributedParallel，\n",
    "# 后者一般用于多主机多GPU，当然也可用于单机多GPU。使用多卡训练的方式有很多，\n",
    "# 当然前提是我们的设备中存在两个及以上的GPU。\n",
    "# 使用时直接用model传入torch.nn.DataParallel函数即可\n",
    "\n",
    "# 对模型\n",
    "net = torch.nn.DataParallel(model) # 这时候，默认所有显卡都会被使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你的电脑有很多显卡，但只想利用其中一部分，如只使用编号为0、1、3、4的4个GPU\n",
    "# 假设有4个gpu，id设置如下\n",
    "device_ids = [0,1,2,3]\n",
    "# 对数据\n",
    "input_data = input_data.to(device=device_ids[0])\n",
    "# 对于模型\n",
    "net = torch.nn.DataParallel(model)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str,[0,1,2,3]))\n",
    "net = torch.nn.DataParallel(model)\n",
    "# 其中CUDA_VISIBLE_DEVICES表示当前可以被PyTorch程序检测到的GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单机多gpu的实现代码\n",
    "# 1.加载数据\n",
    "boston = load_boston()\n",
    "X, y = (boston.data, boston.target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# 组合训练数据及标签\n",
    "# zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表\n",
    "mytest = list(zip(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.把数据转换为批处理加载方式 批次为128，打乱数据\n",
    "from torch.utils import data\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.FloatTensor\n",
    "train_loader = data.DataLoader(myset, batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.定义网络\n",
    "class Net1(nn.Module):\n",
    "    \"\"\"\n",
    "    使用sequential构建网络\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(Net1, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(nn.Linear(in_dim, n_hidden_1))\n",
    "        self.layer2 = torch.nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2))\n",
    "        self.layer3 = torch.nn.Sequential(nn.Linear(n_hideen_2, out_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.layer1(x))\n",
    "        x1 = F.relu(self.layer2(x1))\n",
    "        x2 = self.layer3(x1)\n",
    "        # 显示每个gpu分配的数据大小\n",
    "        print(\"\\tIn Model: input size\", x.size(), \"output size\", x2.size())\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.把模型转换为多gpu并发处理格式\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 实例化网络\n",
    "model = Net1(13,16,2,1)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let us use\", torch.cuda.device_count(), \"GPUS\")\n",
    "    # dim = 0 [64, xxx] -> [32,...],[32,...] on 2GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.选择优化器及损失函数\n",
    "optimizer_orig = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.模型训练，并可视化损失值\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for data, label in train_loader:\n",
    "        input = data.type(dtype).to(device)\n",
    "        label = label.type(dtype).to(device)\n",
    "        output = loss_func(output, label)\n",
    "        # 反向传播\n",
    "        optimizer_orig.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_orig.step()\n",
    "        print(\"Outside: input size\", input.size(), \"output_size\", output.size())\n",
    "        writer.add_scalar('train_loss_paral', loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单机多GPU也可使用DistributedParallel，它多用于分布式训练，但也可以用在单机多GPU的训练，\n",
    "# 配置比使用nn.DataParallel稍微麻烦一点，但是训练速度和效果更好一点。具体配置为\n",
    "\n",
    "# 初始化使用nccl后端\n",
    "torch.distributed.init_process_group(backend=\"nccl\")\n",
    "# 模型并行化\n",
    "model = torch.nn.parallel.DistributedDataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单机运行时使用下列方法启动:\n",
    "# python -m torch.distributed.launch main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GPU可以提升训练的速度，但如果使用不当，可能影响使用效率，具体使用时要注意以下几点：\n",
    "#  1）GPU的数量尽量为偶数，奇数的GPU有可能会出现异常中断的情况；\n",
    "#  2）GPU很快，但数据量较小时，效果可能没有单GPU好，甚至还不如CPU；\n",
    "#  3）如果内存不够大，使用多GPU训练的时候可设置pin_memory为False，当然使用精度稍微低一点的数据类型有时也有效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
